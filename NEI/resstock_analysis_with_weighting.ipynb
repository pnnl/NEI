{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # built-in functions for working with the Windows system\n",
    "import pandas as pd # most popular data science library for working with panel/tabular data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path for users\n",
    "username = os.getlogin()\n",
    "if username == \"root\":\n",
    "    username = os.getenv(\"USER\")\n",
    "\n",
    "if username == \"rose775\":\n",
    "    path_2024_1 = \"/Users/rose775/Library/CloudStorage/OneDrive-PNNL/General - NEB Decarb/Datasets/ResStock/2024.1\"\n",
    "    path_2024_2 = \"/Users/rose775/Library/CloudStorage/OneDrive-PNNL/General - NEB Decarb/Datasets/ResStock/2024.2\"\n",
    "    path_out_2024_1 = \"/Users/rose775/Library/CloudStorage/OneDrive-PNNL/General - NEB Decarb/Analysis/resstock_results_2/2024_1\"\n",
    "    path_out_2024_2 = \"/Users/rose775/Library/CloudStorage/OneDrive-PNNL/General - NEB Decarb/Analysis/resstock_results_2/2024_2\"\n",
    "\n",
    "elif username == \"kieren_username\":\n",
    "    path_2024_1 = \"/General - NEB Decarb/Datasets/ResStock/2024.1\"\n",
    "    path_2024_2 = \"/General - NEB Decarb/Datasets/ResStock/2024.2\"\n",
    "    path_out_2024_1 = \"/General - NEB Decarb/Analysis/resstock_results_2/2024_1\"\n",
    "    path_out_2024_2 = \"/General - NEB Decarb/Analysis/resstock_results_2/2024_2\"\n",
    "\n",
    "elif username == \"max_username\":\n",
    "    path_2024_1 = \"/General - NEB Decarb/Datasets/ResStock/2024.1\"\n",
    "    path_2024_2 = \"/General - NEB Decarb/Datasets/ResStock/2024.2\"\n",
    "    path_out_2024_1 = \"/General - NEB Decarb/Analysis/resstock_results_2/2024_1\"\n",
    "    path_out_2024_2 = \"/General - NEB Decarb/Analysis/resstock_results_2/2024_2\"\n",
    "\n",
    "else:\n",
    "    print(\"Who are you and why are you doing ResStock analysis?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_2024_1 = [\"baseline_metadata_and_annual_results.parquet\", \"upgrade1.04_metadata_and_annual_results.parquet\", \"upgrade2.01_metadata_and_annual_results.parquet\",\n",
    "                \"upgrade2.02_metadata_and_annual_results.parquet\", \"upgrade2.03_metadata_and_annual_results.parquet\", \"upgrade2.04_metadata_and_annual_results.parquet\",\n",
    "                \"upgrade2.05_metadata_and_annual_results.parquet\", \"upgrade3.03_metadata_and_annual_results.parquet\", \"upgrade3.07_metadata_and_annual_results.parquet\",\n",
    "                \"upgrade4.04_metadata_and_annual_results.parquet\"]\n",
    "\n",
    "files_2024_2 = [\"baseline_metadata_and_annual_results.parquet\", \"upgrade01_metadata_and_annual_results.parquet\", \"upgrade02_metadata_and_annual_results.parquet\",\n",
    "                \"upgrade03_metadata_and_annual_results.parquet\", \"upgrade04_metadata_and_annual_results.parquet\", \"upgrade05_metadata_and_annual_results.parquet\",\n",
    "                \"upgrade06_metadata_and_annual_results.parquet\", \"upgrade07_metadata_and_annual_results.parquet\", \"upgrade08_metadata_and_annual_results.parquet\",\n",
    "                \"upgrade09_metadata_and_annual_results.parquet\", \"upgrade10_metadata_and_annual_results.parquet\", \"upgrade14_metadata_and_annual_results.parquet\",\n",
    "                \"upgrade15_metadata_and_annual_results.parquet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_resstock_files(files, path_in):\n",
    "    dfs = {} # create empty dictionary to fill with Pandas dataframes\n",
    "\n",
    "    # Loop through each file name, read the file, and store the df in dfs dictionary\n",
    "    if files is files_2024_1: # make sure we\"re reading files from 2024v1\n",
    "        for file in files:\n",
    "            x = file.split(\"_\")[0] # just use the \"baseline\" or \"upgradeXX\" information for naming the dfs\n",
    "            df_name = f\"{x}_2024_1\"\n",
    "            dfs[df_name] = pd.read_parquet(f\"{path_in}/{file}\").reset_index() # read the parquet and save the named df to the dict\n",
    "    elif files is files_2024_2:\n",
    "        for file in files:\n",
    "            x = file.split(\"_\")[0]\n",
    "            df_name = f\"{x}_2024_2\"\n",
    "            dfs[df_name] = pd.read_parquet(f\"{path_in}/{file}\").reset_index()\n",
    "\n",
    "    keys = list(dfs.keys())\n",
    "\n",
    "    return dfs, keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_diff(df, baseline):\n",
    "    # Step 1: Identify numeric columns in both DataFrames\n",
    "    numeric_cols_df = df.select_dtypes(include=\"number\").columns\n",
    "    numeric_cols_baseline = baseline.select_dtypes(include=\"number\").columns\n",
    "\n",
    "    # Exclude \"weight\" and \"in.sqft\" from the numeric columns\n",
    "    cols_to_exclude = [\"weight\", \"in.sqft\"]\n",
    "    numeric_cols_df = numeric_cols_df.difference(cols_to_exclude)\n",
    "    numeric_cols_baseline = numeric_cols_baseline.difference(cols_to_exclude)\n",
    "\n",
    "    # Step 2: Subtract matching numeric columns excluding \"weight\" and \"in.sqft\"\n",
    "    diff_df = df.copy()  # Make a copy of df to store the differences\n",
    "    for col in numeric_cols_df.intersection(numeric_cols_baseline):\n",
    "        diff_df[col] -= baseline[col]\n",
    "\n",
    "    # Step 3: Add \"weight\" and \"in.sqft\" back to the DataFrame\n",
    "    diff_df[cols_to_exclude] = df[cols_to_exclude]\n",
    "\n",
    "    # Step 4: Create a new DataFrame with the differences including \"weight\" and \"in.sqft\"\n",
    "    diff_df = diff_df.assign(**{col: diff_df[col] for col in numeric_cols_df})\n",
    "\n",
    "    return diff_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resstock_summary(df_dict, path_out):\n",
    "    baseline_key = [key for key in df_dict.keys() if \"baseline\" in key][0]\n",
    "    keys = [key for key in df_dict.keys() if key != baseline_key]\n",
    "    baseline = df_dict[baseline_key]\n",
    "    for key in keys:\n",
    "        # resstock measure\n",
    "        df = df_dict[key]\n",
    "        \n",
    "        # get difference between baseline and measure dfs (df_measure - df_baseline)\n",
    "        diff_df = df_diff(df, baseline)\n",
    "\n",
    "        # calculate the weighted values by multiplying row[\"weight\"] by row[numerical_column]\n",
    "        df_weighted = diff_df.copy()\n",
    "        numerical_cols = df_weighted.select_dtypes(include=np.number).drop(columns=[\"weight\", \"in.sqft\"]).columns\n",
    "        df_weighted[numerical_cols] = df_weighted[numerical_cols].mul(df_weighted[\"weight\"], axis=0)\n",
    "\n",
    "        # aggregate weighted means and unweighted sums\n",
    "        conditions = [\"out.emissions_reduction.all_fuels\", \"in.sqft\", \"bldg_id\", \"out.site_energy.net.energy_consumption.kwh\"]\n",
    "        numerical_cols = df_weighted.select_dtypes(include=np.number).columns\n",
    "        agg_dict_1 = {col: \"mean\" for col in numerical_cols if any(condition in col.lower() for condition in conditions)}\n",
    "        agg_dict_1[\"bldg_id\"] = \"count\"\n",
    "        agg_dict_1[\"in.county\"] = \"nunique\"\n",
    "        agg_dict_2 = {col: \"sum\" for col in numerical_cols if any(condition in col.lower() for condition in conditions)}\n",
    "\n",
    "        unweighted_mean = diff_df.groupby(\"in.ashrae_iecc_climate_zone_2004\").agg(agg_dict_1).reset_index()\n",
    "        weighted_sum = df_weighted.groupby(\"in.ashrae_iecc_climate_zone_2004\").agg(agg_dict_2).reset_index()\n",
    "\n",
    "        # merge for output\n",
    "        result = pd.merge(unweighted_mean, weighted_sum, on = \"in.ashrae_iecc_climate_zone_2004\", suffixes = (\"_avg_site\", \"_entire_cz\"))\n",
    "        \n",
    "        # to .csv\n",
    "        result.to_csv(f\"{path_out}/{key}_results.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carbon analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first do the 2024v1 files\n",
    "dfs_2024_1 = read_resstock_files(files_2024_1, path_2024_1)[0]\n",
    "resstock_summary(dfs_2024_1, path_out_2024_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the 2024v1 dfs to clear up some RAM\n",
    "del dfs_2024_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now do the 2024v2 files\n",
    "dfs_2024_2 = read_resstock_files(files_2024_2, path_2024_2)[0]\n",
    "resstock_summary(dfs_2024_2, path_out_2024_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Population table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_2024_1 = read_resstock_files(files_2024_1, path_2024_1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_2024_1[\"baseline_2024_1\"].groupby(\"in.ashrae_iecc_climate_zone_2004\").agg({\"in.county\": \"nunique\",\n",
    "                                                                                  \"weight\": \"sum\"\n",
    "                                                                                  }).round(0).to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dfs_2024_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_2024_2 = read_resstock_files(files_2024_2, path_2024_2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_2024_2[\"baseline_2024_2\"].groupby(\"in.ashrae_iecc_climate_zone_2004\").agg({\"in.county\": \"nunique\",\n",
    "                                                                                  \"weight\": \"sum\"}).round(0).to_clipboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
